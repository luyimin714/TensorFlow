{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-labels-idx1-ubyte.gz\n",
      "0 Train acc: 0.88 Test acc: 0.8984\n",
      "1 Train acc: 0.9 Test acc: 0.9157\n",
      "2 Train acc: 0.9 Test acc: 0.9212\n",
      "3 Train acc: 0.96 Test acc: 0.9254\n",
      "4 Train acc: 0.94 Test acc: 0.9318\n",
      "5 Train acc: 0.9 Test acc: 0.9344\n",
      "6 Train acc: 0.94 Test acc: 0.9392\n",
      "7 Train acc: 0.94 Test acc: 0.9436\n",
      "8 Train acc: 0.88 Test acc: 0.9469\n",
      "9 Train acc: 0.92 Test acc: 0.9506\n",
      "10 Train acc: 0.96 Test acc: 0.9524\n",
      "11 Train acc: 0.92 Test acc: 0.9567\n",
      "12 Train acc: 0.98 Test acc: 0.9587\n",
      "13 Train acc: 0.92 Test acc: 0.9588\n",
      "14 Train acc: 1.0 Test acc: 0.9611\n",
      "15 Train acc: 0.98 Test acc: 0.9624\n",
      "16 Train acc: 0.98 Test acc: 0.9634\n",
      "17 Train acc: 0.96 Test acc: 0.965\n",
      "18 Train acc: 1.0 Test acc: 0.9655\n",
      "19 Train acc: 0.98 Test acc: 0.9663\n",
      "20 Train acc: 1.0 Test acc: 0.9663\n",
      "21 Train acc: 1.0 Test acc: 0.967\n",
      "22 Train acc: 1.0 Test acc: 0.9682\n",
      "23 Train acc: 1.0 Test acc: 0.9705\n",
      "24 Train acc: 1.0 Test acc: 0.9701\n",
      "25 Train acc: 1.0 Test acc: 0.9698\n",
      "26 Train acc: 1.0 Test acc: 0.9702\n",
      "27 Train acc: 0.98 Test acc: 0.9707\n",
      "28 Train acc: 0.98 Test acc: 0.9715\n",
      "29 Train acc: 0.98 Test acc: 0.9731\n",
      "30 Train acc: 1.0 Test acc: 0.973\n",
      "31 Train acc: 1.0 Test acc: 0.9746\n",
      "32 Train acc: 0.98 Test acc: 0.9735\n",
      "33 Train acc: 0.98 Test acc: 0.9732\n",
      "34 Train acc: 0.98 Test acc: 0.9746\n",
      "35 Train acc: 0.98 Test acc: 0.9736\n",
      "36 Train acc: 0.98 Test acc: 0.974\n",
      "37 Train acc: 1.0 Test acc: 0.9759\n",
      "38 Train acc: 1.0 Test acc: 0.9753\n",
      "39 Train acc: 1.0 Test acc: 0.9756\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, kernel_initializer=he_init, name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../dataset/mnist/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "\"\"\"\n",
    "UPDATE_OPS is a collection of ops (operations performed when the graph runs, like multiplication, ReLU, etc.), not variables. \n",
    "Specifically, this collection maintains a list of ops which need to run after every training step.\n",
    "\"\"\"\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\"\"\"\n",
    "训练时，需要更新moving_mean和moving_variance。\n",
    "默认情况下，更新操作被放入tf.GraphKeys.UPDATE_OPS，\n",
    "因此需要将它们作为依赖项添加到train_op。此外，\n",
    "在获取update_ops集合之前，请务必添加batch_normalization操作。\n",
    "否则，update_ops将为空，并且训练/推断将无法正常工作。\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], \n",
    "                     feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train acc:\", acc_train, \"Test acc:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-cb4ef1dca3b0>:60: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../dataset/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0 Train acc: 0.88 Test acc: 0.8984\n",
      "1 Train acc: 0.9 Test acc: 0.9157\n",
      "2 Train acc: 0.9 Test acc: 0.9212\n",
      "3 Train acc: 0.96 Test acc: 0.9254\n",
      "4 Train acc: 0.94 Test acc: 0.9318\n",
      "5 Train acc: 0.9 Test acc: 0.9344\n",
      "6 Train acc: 0.94 Test acc: 0.9392\n",
      "7 Train acc: 0.94 Test acc: 0.9436\n",
      "8 Train acc: 0.88 Test acc: 0.9469\n",
      "9 Train acc: 0.92 Test acc: 0.9506\n",
      "10 Train acc: 0.96 Test acc: 0.9524\n",
      "11 Train acc: 0.92 Test acc: 0.9567\n",
      "12 Train acc: 0.98 Test acc: 0.9587\n",
      "13 Train acc: 0.92 Test acc: 0.9588\n",
      "14 Train acc: 1.0 Test acc: 0.9611\n",
      "15 Train acc: 0.98 Test acc: 0.9624\n",
      "16 Train acc: 0.98 Test acc: 0.9634\n",
      "17 Train acc: 0.96 Test acc: 0.965\n",
      "18 Train acc: 1.0 Test acc: 0.9655\n",
      "19 Train acc: 0.98 Test acc: 0.9663\n",
      "20 Train acc: 1.0 Test acc: 0.9663\n",
      "21 Train acc: 1.0 Test acc: 0.967\n",
      "22 Train acc: 1.0 Test acc: 0.9682\n",
      "23 Train acc: 1.0 Test acc: 0.9705\n",
      "24 Train acc: 1.0 Test acc: 0.9701\n",
      "25 Train acc: 1.0 Test acc: 0.9698\n",
      "26 Train acc: 1.0 Test acc: 0.9702\n",
      "27 Train acc: 0.98 Test acc: 0.9707\n",
      "28 Train acc: 0.98 Test acc: 0.9715\n",
      "29 Train acc: 0.98 Test acc: 0.9731\n",
      "30 Train acc: 1.0 Test acc: 0.973\n",
      "31 Train acc: 1.0 Test acc: 0.9746\n",
      "32 Train acc: 0.98 Test acc: 0.9735\n",
      "33 Train acc: 0.98 Test acc: 0.9732\n",
      "34 Train acc: 0.98 Test acc: 0.9746\n",
      "35 Train acc: 0.98 Test acc: 0.9736\n",
      "36 Train acc: 0.98 Test acc: 0.974\n",
      "37 Train acc: 1.0 Test acc: 0.9759\n",
      "38 Train acc: 1.0 Test acc: 0.9753\n",
      "39 Train acc: 1.0 Test acc: 0.9756\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\"\"\"\n",
    "partial函数的作用：将所作用的函数作为partial（）函数的第一个参数，\n",
    "原函数的各个参数依次作为partial（）函数的后续参数，\n",
    "原函数有关键字参数的一定要带上关键字，没有的话，按原有参数顺序进行补充。\n",
    "\"\"\"\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "    \n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../dataset/mnist/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], \n",
    "                     feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train acc:\", acc_train, \"Test acc:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'update_ops'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GraphKeys.UPDATE_OPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'dnn/batch_normalization/cond_2/Merge' type=Merge>,\n",
       " <tf.Operation 'dnn/batch_normalization/cond_3/Merge' type=Merge>,\n",
       " <tf.Operation 'dnn/batch_normalization_1/cond_2/Merge' type=Merge>,\n",
       " <tf.Operation 'dnn/batch_normalization_1/cond_3/Merge' type=Merge>,\n",
       " <tf.Operation 'dnn/batch_normalization_2/cond_2/Merge' type=Merge>,\n",
       " <tf.Operation 'dnn/batch_normalization_2/cond_3/Merge' type=Merge>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-labels-idx1-ubyte.gz\n",
      "0 Train acc: 0.86 Test acc: 0.8831\n",
      "1 Train acc: 0.96 Test acc: 0.9262\n",
      "2 Train acc: 0.94 Test acc: 0.9445\n",
      "3 Train acc: 0.96 Test acc: 0.951\n",
      "4 Train acc: 0.98 Test acc: 0.9562\n",
      "5 Train acc: 0.94 Test acc: 0.9596\n",
      "6 Train acc: 1.0 Test acc: 0.9643\n",
      "7 Train acc: 0.96 Test acc: 0.9647\n",
      "8 Train acc: 0.96 Test acc: 0.9678\n",
      "9 Train acc: 1.0 Test acc: 0.9667\n",
      "10 Train acc: 1.0 Test acc: 0.97\n",
      "11 Train acc: 0.98 Test acc: 0.9694\n",
      "12 Train acc: 1.0 Test acc: 0.9735\n",
      "13 Train acc: 1.0 Test acc: 0.9692\n",
      "14 Train acc: 1.0 Test acc: 0.9729\n",
      "15 Train acc: 1.0 Test acc: 0.9738\n",
      "16 Train acc: 1.0 Test acc: 0.9728\n",
      "17 Train acc: 1.0 Test acc: 0.9683\n",
      "18 Train acc: 1.0 Test acc: 0.973\n",
      "19 Train acc: 1.0 Test acc: 0.9744\n",
      "20 Train acc: 1.0 Test acc: 0.973\n",
      "21 Train acc: 1.0 Test acc: 0.9708\n",
      "22 Train acc: 1.0 Test acc: 0.9757\n",
      "23 Train acc: 1.0 Test acc: 0.9753\n",
      "24 Train acc: 1.0 Test acc: 0.9738\n",
      "25 Train acc: 1.0 Test acc: 0.9748\n",
      "26 Train acc: 1.0 Test acc: 0.9751\n",
      "27 Train acc: 1.0 Test acc: 0.9742\n",
      "28 Train acc: 1.0 Test acc: 0.9751\n",
      "29 Train acc: 1.0 Test acc: 0.9759\n",
      "30 Train acc: 1.0 Test acc: 0.976\n",
      "31 Train acc: 1.0 Test acc: 0.9747\n",
      "32 Train acc: 1.0 Test acc: 0.9757\n",
      "33 Train acc: 1.0 Test acc: 0.9752\n",
      "34 Train acc: 1.0 Test acc: 0.9761\n",
      "35 Train acc: 1.0 Test acc: 0.9761\n",
      "36 Train acc: 1.0 Test acc: 0.9757\n",
      "37 Train acc: 1.0 Test acc: 0.9761\n",
      "38 Train acc: 1.0 Test acc: 0.976\n",
      "39 Train acc: 1.0 Test acc: 0.9754\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../dataset/mnist/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, \n",
    "                     feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train acc:\", acc_train, \"Test acc:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重用TensorFlow模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../dataset/mnist/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/my_model_final.ckpt\n",
      "0 Train acc: 1.0 Test acc: 0.9763\n",
      "1 Train acc: 1.0 Test acc: 0.9758\n",
      "2 Train acc: 1.0 Test acc: 0.976\n",
      "3 Train acc: 1.0 Test acc: 0.9759\n",
      "4 Train acc: 1.0 Test acc: 0.9758\n",
      "5 Train acc: 1.0 Test acc: 0.976\n",
      "6 Train acc: 1.0 Test acc: 0.9756\n",
      "7 Train acc: 1.0 Test acc: 0.9761\n",
      "8 Train acc: 1.0 Test acc: 0.976\n",
      "9 Train acc: 1.0 Test acc: 0.9763\n",
      "10 Train acc: 1.0 Test acc: 0.9761\n",
      "11 Train acc: 1.0 Test acc: 0.9761\n",
      "12 Train acc: 1.0 Test acc: 0.9762\n",
      "13 Train acc: 1.0 Test acc: 0.9759\n",
      "14 Train acc: 1.0 Test acc: 0.976\n",
      "15 Train acc: 1.0 Test acc: 0.9764\n",
      "16 Train acc: 1.0 Test acc: 0.9761\n",
      "17 Train acc: 1.0 Test acc: 0.9762\n",
      "18 Train acc: 1.0 Test acc: 0.9761\n",
      "19 Train acc: 1.0 Test acc: 0.9764\n",
      "20 Train acc: 1.0 Test acc: 0.9757\n",
      "21 Train acc: 1.0 Test acc: 0.976\n",
      "22 Train acc: 1.0 Test acc: 0.9758\n",
      "23 Train acc: 1.0 Test acc: 0.9761\n",
      "24 Train acc: 1.0 Test acc: 0.9763\n",
      "25 Train acc: 1.0 Test acc: 0.9761\n",
      "26 Train acc: 1.0 Test acc: 0.976\n",
      "27 Train acc: 1.0 Test acc: 0.9758\n",
      "28 Train acc: 1.0 Test acc: 0.976\n",
      "29 Train acc: 1.0 Test acc: 0.9761\n",
      "30 Train acc: 1.0 Test acc: 0.9759\n",
      "31 Train acc: 1.0 Test acc: 0.9761\n",
      "32 Train acc: 1.0 Test acc: 0.976\n",
      "33 Train acc: 1.0 Test acc: 0.9761\n",
      "34 Train acc: 1.0 Test acc: 0.9762\n",
      "35 Train acc: 1.0 Test acc: 0.9762\n",
      "36 Train acc: 1.0 Test acc: 0.976\n",
      "37 Train acc: 1.0 Test acc: 0.9759\n",
      "38 Train acc: 1.0 Test acc: 0.976\n",
      "39 Train acc: 1.0 Test acc: 0.9759\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, \"./model/my_model_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, \n",
    "                     feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train acc:\", acc_train, \"Test acc:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重用原有模型的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/my_model_final.ckpt\n",
      "0 Train acc: 0.98 Test acc: 0.967\n",
      "1 Train acc: 1.0 Test acc: 0.9725\n",
      "2 Train acc: 1.0 Test acc: 0.9758\n",
      "3 Train acc: 1.0 Test acc: 0.9765\n",
      "4 Train acc: 1.0 Test acc: 0.9759\n",
      "5 Train acc: 1.0 Test acc: 0.9766\n",
      "6 Train acc: 1.0 Test acc: 0.9774\n",
      "7 Train acc: 1.0 Test acc: 0.9773\n",
      "8 Train acc: 1.0 Test acc: 0.9763\n",
      "9 Train acc: 1.0 Test acc: 0.9772\n",
      "10 Train acc: 1.0 Test acc: 0.9775\n",
      "11 Train acc: 1.0 Test acc: 0.9765\n",
      "12 Train acc: 1.0 Test acc: 0.9774\n",
      "13 Train acc: 1.0 Test acc: 0.9772\n",
      "14 Train acc: 1.0 Test acc: 0.9763\n",
      "15 Train acc: 1.0 Test acc: 0.9782\n",
      "16 Train acc: 1.0 Test acc: 0.9777\n",
      "17 Train acc: 1.0 Test acc: 0.977\n",
      "18 Train acc: 1.0 Test acc: 0.9779\n",
      "19 Train acc: 1.0 Test acc: 0.9778\n",
      "20 Train acc: 1.0 Test acc: 0.9771\n",
      "21 Train acc: 1.0 Test acc: 0.9774\n",
      "22 Train acc: 1.0 Test acc: 0.9777\n",
      "23 Train acc: 1.0 Test acc: 0.9776\n",
      "24 Train acc: 1.0 Test acc: 0.9774\n",
      "25 Train acc: 1.0 Test acc: 0.9775\n",
      "26 Train acc: 1.0 Test acc: 0.9771\n",
      "27 Train acc: 1.0 Test acc: 0.9772\n",
      "28 Train acc: 1.0 Test acc: 0.9769\n",
      "29 Train acc: 1.0 Test acc: 0.9776\n",
      "30 Train acc: 1.0 Test acc: 0.9766\n",
      "31 Train acc: 1.0 Test acc: 0.9778\n",
      "32 Train acc: 1.0 Test acc: 0.9776\n",
      "33 Train acc: 1.0 Test acc: 0.9775\n",
      "34 Train acc: 1.0 Test acc: 0.9775\n",
      "35 Train acc: 1.0 Test acc: 0.9769\n",
      "36 Train acc: 1.0 Test acc: 0.978\n",
      "37 Train acc: 1.0 Test acc: 0.9773\n",
      "38 Train acc: 1.0 Test acc: 0.977\n",
      "39 Train acc: 1.0 Test acc: 0.9771\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./model/my_model_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, \n",
    "                     feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train acc:\", acc_train, \"Test acc:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重用其他框架的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 冻结低层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/my_model_final.ckpt\n",
      "0 Train acc: 0.98 Test acc: 0.9753\n",
      "1 Train acc: 1.0 Test acc: 0.976\n",
      "2 Train acc: 1.0 Test acc: 0.9773\n",
      "3 Train acc: 1.0 Test acc: 0.9767\n",
      "4 Train acc: 1.0 Test acc: 0.9764\n",
      "5 Train acc: 1.0 Test acc: 0.9771\n",
      "6 Train acc: 1.0 Test acc: 0.9771\n",
      "7 Train acc: 1.0 Test acc: 0.9771\n",
      "8 Train acc: 1.0 Test acc: 0.9776\n",
      "9 Train acc: 1.0 Test acc: 0.9768\n",
      "10 Train acc: 1.0 Test acc: 0.9768\n",
      "11 Train acc: 1.0 Test acc: 0.9769\n",
      "12 Train acc: 1.0 Test acc: 0.9773\n",
      "13 Train acc: 1.0 Test acc: 0.977\n",
      "14 Train acc: 1.0 Test acc: 0.9773\n",
      "15 Train acc: 1.0 Test acc: 0.9772\n",
      "16 Train acc: 1.0 Test acc: 0.9774\n",
      "17 Train acc: 1.0 Test acc: 0.9771\n",
      "18 Train acc: 1.0 Test acc: 0.9767\n",
      "19 Train acc: 1.0 Test acc: 0.9771\n",
      "20 Train acc: 1.0 Test acc: 0.977\n",
      "21 Train acc: 1.0 Test acc: 0.9773\n",
      "22 Train acc: 1.0 Test acc: 0.9769\n",
      "23 Train acc: 1.0 Test acc: 0.977\n",
      "24 Train acc: 1.0 Test acc: 0.9772\n",
      "25 Train acc: 1.0 Test acc: 0.9771\n",
      "26 Train acc: 1.0 Test acc: 0.9769\n",
      "27 Train acc: 1.0 Test acc: 0.977\n",
      "28 Train acc: 1.0 Test acc: 0.9771\n",
      "29 Train acc: 1.0 Test acc: 0.9768\n",
      "30 Train acc: 1.0 Test acc: 0.9769\n",
      "31 Train acc: 1.0 Test acc: 0.9769\n",
      "32 Train acc: 1.0 Test acc: 0.9769\n",
      "33 Train acc: 1.0 Test acc: 0.9772\n",
      "34 Train acc: 1.0 Test acc: 0.9771\n",
      "35 Train acc: 1.0 Test acc: 0.9769\n",
      "36 Train acc: 1.0 Test acc: 0.977\n",
      "37 Train acc: 1.0 Test acc: 0.977\n",
      "38 Train acc: 1.0 Test acc: 0.9767\n",
      "39 Train acc: 1.0 Test acc: 0.9773\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./model/my_model_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train acc:\", acc_train, \"Test acc:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 缓存冻结层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/my_model_final.ckpt\n",
      "0 Validation acc: 0.9774\n",
      "1 Validation acc: 0.9776\n",
      "2 Validation acc: 0.9764\n",
      "3 Validation acc: 0.978\n",
      "4 Validation acc: 0.9766\n",
      "5 Validation acc: 0.9774\n",
      "6 Validation acc: 0.9772\n",
      "7 Validation acc: 0.977\n",
      "8 Validation acc: 0.9774\n",
      "9 Validation acc: 0.9778\n",
      "10 Validation acc: 0.9778\n",
      "11 Validation acc: 0.978\n",
      "12 Validation acc: 0.9778\n",
      "13 Validation acc: 0.9778\n",
      "14 Validation acc: 0.978\n",
      "15 Validation acc: 0.9778\n",
      "16 Validation acc: 0.9778\n",
      "17 Validation acc: 0.9778\n",
      "18 Validation acc: 0.9778\n",
      "19 Validation acc: 0.9776\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 50\n",
    "n_epochs = 20\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    \n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./model/my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y: y_valid})\n",
    "        print(epoch, \"Validation acc:\", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习速率调度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1 / 10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps,\n",
    "                                               decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training acc: 0.98 Validation acc: 0.9648\n",
      "1 Training acc: 0.96 Validation acc: 0.9748\n",
      "2 Training acc: 0.98 Validation acc: 0.978\n",
      "3 Training acc: 1.0 Validation acc: 0.9804\n",
      "4 Training acc: 0.98 Validation acc: 0.9814\n",
      "5 Training acc: 1.0 Validation acc: 0.9852\n",
      "6 Training acc: 1.0 Validation acc: 0.9852\n",
      "7 Training acc: 1.0 Validation acc: 0.9838\n",
      "8 Training acc: 1.0 Validation acc: 0.9838\n",
      "9 Training acc: 1.0 Validation acc: 0.984\n",
      "10 Training acc: 1.0 Validation acc: 0.9836\n",
      "11 Training acc: 1.0 Validation acc: 0.9838\n",
      "12 Training acc: 1.0 Validation acc: 0.9836\n",
      "13 Training acc: 1.0 Validation acc: 0.9838\n",
      "14 Training acc: 1.0 Validation acc: 0.9836\n",
      "15 Training acc: 1.0 Validation acc: 0.9836\n",
      "16 Training acc: 1.0 Validation acc: 0.9836\n",
      "17 Training acc: 1.0 Validation acc: 0.9836\n",
      "18 Training acc: 1.0 Validation acc: 0.9836\n",
      "19 Training acc: 1.0 Validation acc: 0.9836\n",
      "20 Training acc: 1.0 Validation acc: 0.9836\n",
      "21 Training acc: 1.0 Validation acc: 0.9836\n",
      "22 Training acc: 1.0 Validation acc: 0.9836\n",
      "23 Training acc: 1.0 Validation acc: 0.9836\n",
      "24 Training acc: 1.0 Validation acc: 0.9836\n",
      "25 Training acc: 1.0 Validation acc: 0.9836\n",
      "26 Training acc: 1.0 Validation acc: 0.9836\n",
      "27 Training acc: 1.0 Validation acc: 0.9836\n",
      "28 Training acc: 0.98 Validation acc: 0.9836\n",
      "29 Training acc: 1.0 Validation acc: 0.9836\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Training acc:\", acc, \"Validation acc:\", acc_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提前停止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1和l2正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(784, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/kernel:0' shape=(300, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/kernel/Momentum:0' shape=(784, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias/Momentum:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/kernel/Momentum:0' shape=(300, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/bias/Momentum:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training acc: 0.895 Validation acc: 0.9064\n",
      "1 Training acc: 0.91 Validation acc: 0.9044\n",
      "2 Training acc: 0.875 Validation acc: 0.9026\n",
      "3 Training acc: 0.89 Validation acc: 0.907\n",
      "4 Training acc: 0.935 Validation acc: 0.908\n",
      "5 Training acc: 0.905 Validation acc: 0.9134\n",
      "6 Training acc: 0.92 Validation acc: 0.9196\n",
      "7 Training acc: 0.915 Validation acc: 0.9198\n",
      "8 Training acc: 0.93 Validation acc: 0.9222\n",
      "9 Training acc: 0.91 Validation acc: 0.926\n",
      "10 Training acc: 0.915 Validation acc: 0.9274\n",
      "11 Training acc: 0.925 Validation acc: 0.9284\n",
      "12 Training acc: 0.875 Validation acc: 0.9304\n",
      "13 Training acc: 0.935 Validation acc: 0.9316\n",
      "14 Training acc: 0.94 Validation acc: 0.9316\n",
      "15 Training acc: 0.915 Validation acc: 0.9316\n",
      "16 Training acc: 0.96 Validation acc: 0.9336\n",
      "17 Training acc: 0.94 Validation acc: 0.9352\n",
      "18 Training acc: 0.88 Validation acc: 0.9362\n",
      "19 Training acc: 0.905 Validation acc: 0.9342\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) \n",
    "\n",
    "learning_rate = 0.01    \n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "#     initial_learning_rate = 0.1\n",
    "#     decay_steps = 10000\n",
    "#     decay_rate = 1 / 10\n",
    "#     global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "#     learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "#                                                global_step,\n",
    "#                                                decay_steps,\n",
    "#                                                decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss) #, global_step=global_step\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Training acc:\", acc, \"Validation acc:\", acc_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 层数多的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001\n",
    "\n",
    "my_dense_layer = partial(tf.layers.dense,\n",
    "                         activation=tf.nn.relu,\n",
    "                         kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "# W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training acc: 0.93 Validation acc: 0.9152\n",
      "1 Training acc: 0.9 Validation acc: 0.9202\n",
      "2 Training acc: 0.9 Validation acc: 0.9152\n",
      "3 Training acc: 0.9 Validation acc: 0.923\n",
      "4 Training acc: 0.95 Validation acc: 0.9284\n",
      "5 Training acc: 0.92 Validation acc: 0.9316\n",
      "6 Training acc: 0.955 Validation acc: 0.9382\n",
      "7 Training acc: 0.93 Validation acc: 0.9402\n",
      "8 Training acc: 0.95 Validation acc: 0.9462\n",
      "9 Training acc: 0.93 Validation acc: 0.9428\n",
      "10 Training acc: 0.955 Validation acc: 0.9474\n",
      "11 Training acc: 0.96 Validation acc: 0.9484\n",
      "12 Training acc: 0.91 Validation acc: 0.9446\n",
      "13 Training acc: 0.955 Validation acc: 0.9474\n",
      "14 Training acc: 0.955 Validation acc: 0.9514\n",
      "15 Training acc: 0.92 Validation acc: 0.9496\n",
      "16 Training acc: 0.975 Validation acc: 0.9478\n",
      "17 Training acc: 0.955 Validation acc: 0.9516\n",
      "18 Training acc: 0.915 Validation acc: 0.951\n",
      "19 Training acc: 0.93 Validation acc: 0.9478\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) \n",
    "\n",
    "learning_rate = 0.01    \n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss) #, global_step=global_step\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Training acc:\", acc, \"Validation acc:\", acc_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "dropout_rate = 0.5\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training acc: 0.9 Validation acc: 0.9028\n",
      "1 Training acc: 0.92 Validation acc: 0.9232\n",
      "2 Training acc: 0.905 Validation acc: 0.9334\n",
      "3 Training acc: 0.925 Validation acc: 0.943\n",
      "4 Training acc: 0.95 Validation acc: 0.9438\n",
      "5 Training acc: 0.92 Validation acc: 0.953\n",
      "6 Training acc: 0.975 Validation acc: 0.9564\n",
      "7 Training acc: 0.955 Validation acc: 0.9608\n",
      "8 Training acc: 0.975 Validation acc: 0.9626\n",
      "9 Training acc: 0.975 Validation acc: 0.9656\n",
      "10 Training acc: 0.965 Validation acc: 0.967\n",
      "11 Training acc: 0.96 Validation acc: 0.9692\n",
      "12 Training acc: 0.965 Validation acc: 0.97\n",
      "13 Training acc: 0.965 Validation acc: 0.9718\n",
      "14 Training acc: 0.98 Validation acc: 0.971\n",
      "15 Training acc: 0.96 Validation acc: 0.9724\n",
      "16 Training acc: 0.985 Validation acc: 0.9734\n",
      "17 Training acc: 0.975 Validation acc: 0.9736\n",
      "18 Training acc: 0.975 Validation acc: 0.9748\n",
      "19 Training acc: 0.955 Validation acc: 0.9762\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "#     base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01    \n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss) #, global_step=global_step\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        acc = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Training acc:\", acc, \"Validation acc:\", acc_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最大范数正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(784, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/kernel:0' shape=(300, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/kernel:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights1 = tf.clip_by_norm(weights1, clip_norm=threshold, axes=1)\n",
    "clip_weights1 = tf.assign(weights1, clipped_weights1)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation acc: 0.9566\n",
      "1 Validation acc: 0.9696\n",
      "2 Validation acc: 0.9712\n",
      "3 Validation acc: 0.9766\n",
      "4 Validation acc: 0.977\n",
      "5 Validation acc: 0.9776\n",
      "6 Validation acc: 0.9816\n",
      "7 Validation acc: 0.9812\n",
      "8 Validation acc: 0.9798\n",
      "9 Validation acc: 0.9818\n",
      "10 Validation acc: 0.981\n",
      "11 Validation acc: 0.9836\n",
      "12 Validation acc: 0.9822\n",
      "13 Validation acc: 0.9842\n",
      "14 Validation acc: 0.9838\n",
      "15 Validation acc: 0.9838\n",
      "16 Validation acc: 0.9826\n",
      "17 Validation acc: 0.984\n",
      "18 Validation acc: 0.9842\n",
      "19 Validation acc: 0.984\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights1.eval()\n",
    "            clip_weights2.eval()\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation acc:\", acc_valid)\n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1/kernel:0\n",
      "hidden1/bias:0\n",
      "hidden2/kernel:0\n",
      "hidden2/bias:0\n",
      "outputs/kernel:0\n",
      "outputs/bias:0\n",
      "hidden1/kernel/Momentum:0\n",
      "hidden1/bias/Momentum:0\n",
      "hidden2/kernel/Momentum:0\n",
      "hidden2/bias/Momentum:0\n",
      "outputs/kernel/Momentum:0\n",
      "outputs/bias/Momentum:0\n"
     ]
    }
   ],
   "source": [
    "for variable in tf.global_variables():\n",
    "    print(variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\", \n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation acc: 0.9556\n",
      "1 Validation acc: 0.9706\n",
      "2 Validation acc: 0.9682\n",
      "3 Validation acc: 0.9726\n",
      "4 Validation acc: 0.9766\n",
      "5 Validation acc: 0.976\n",
      "6 Validation acc: 0.981\n",
      "7 Validation acc: 0.9798\n",
      "8 Validation acc: 0.9838\n",
      "9 Validation acc: 0.9824\n",
      "10 Validation acc: 0.9814\n",
      "11 Validation acc: 0.9832\n",
      "12 Validation acc: 0.983\n",
      "13 Validation acc: 0.9832\n",
      "14 Validation acc: 0.9838\n",
      "15 Validation acc: 0.9842\n",
      "16 Validation acc: 0.9834\n",
      "17 Validation acc: 0.984\n",
      "18 Validation acc: 0.9838\n",
      "19 Validation acc: 0.9836\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, \n",
    "                              activation=tf.nn.relu, \n",
    "                              kernel_regularizer=max_norm_reg, \n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, \n",
    "                              activation=tf.nn.relu, \n",
    "                              kernel_regularizer=max_norm_reg,\n",
    "                              name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation acc:\", acc_valid)\n",
    "    save_path = saver.save(sess, \"./model/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dnn/hidden1/kernel/Regularizer/Assign:0' shape=(784, 300) dtype=float32_ref>,\n",
       " <tf.Tensor 'dnn/hidden2/kernel/Regularizer/Assign:0' shape=(300, 50) dtype=float32_ref>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(\"max_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
