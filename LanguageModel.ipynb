{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉熵计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32656264 0.4643688 ]\n"
     ]
    }
   ],
   "source": [
    "word_labels = tf.constant([2, 0])\n",
    "predict_logits = tf.constant([[2.0, -1.0, 3.0], [1.0, 0.0, -0.5]])\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=word_labels,\n",
    "                                                      logits=predict_logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32656264 0.4643688 ]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "word_prob_distribution = tf.constant([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])\n",
    "predict_logits = tf.constant([[2.0, -1.0, 3.0], [1.0, 0.0, -0.5]])\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=word_prob_distribution,\n",
    "                                               logits=predict_logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTB数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "\n",
    "RAW_DATA = \"../dataset/simple-examples/data/ptb.train.txt\"\n",
    "VOCAB_OUTPUT = \"ptb.vocab\"\n",
    "\n",
    "counter = collections.Counter() #统计单词出现的频率\n",
    "with codecs.open(RAW_DATA, \"r\", \"utf-8\") as f:\n",
    "    for line in f:\n",
    "        for word in line.strip().split():\n",
    "            counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'creativity': 11,\n",
       "         'summary': 7,\n",
       "         'computer': 420,\n",
       "         '26-week': 5,\n",
       "         'pilson': 30,\n",
       "         'medium': 8,\n",
       "         'hispanics': 7,\n",
       "         'inside': 43,\n",
       "         'mcdonough': 18,\n",
       "         'complains': 13,\n",
       "         'labor-management': 27,\n",
       "         'singled': 8,\n",
       "         'utah': 20,\n",
       "         'lynn': 7,\n",
       "         'arrogant': 6,\n",
       "         'distorted': 6,\n",
       "         'crush': 12,\n",
       "         'die': 20,\n",
       "         'counterparts': 16,\n",
       "         'shoot': 6,\n",
       "         'fda': 26,\n",
       "         'lesson': 13,\n",
       "         'phoenix': 29,\n",
       "         'pit': 11,\n",
       "         'preclude': 6,\n",
       "         'helpful': 9,\n",
       "         'lend': 20,\n",
       "         'u.s.a': 12,\n",
       "         'illegal': 45,\n",
       "         'median': 8,\n",
       "         'powerhouse': 7,\n",
       "         'fanfare': 6,\n",
       "         'insurance': 401,\n",
       "         'killer': 6,\n",
       "         'regarded': 20,\n",
       "         'bolster': 25,\n",
       "         'temple': 15,\n",
       "         'exception': 16,\n",
       "         'lawsuits': 44,\n",
       "         'las': 32,\n",
       "         'designs': 17,\n",
       "         'news': 333,\n",
       "         'tops': 6,\n",
       "         'ted': 14,\n",
       "         'credits': 29,\n",
       "         'original': 67,\n",
       "         'package': 89,\n",
       "         'otherwise': 39,\n",
       "         'members': 237,\n",
       "         'centerpiece': 6,\n",
       "         'enormous': 41,\n",
       "         'band': 18,\n",
       "         'contractor': 25,\n",
       "         'legally': 7,\n",
       "         'consumed': 10,\n",
       "         'arena': 9,\n",
       "         'earthquake': 248,\n",
       "         'singer': 17,\n",
       "         'street': 327,\n",
       "         'swaps': 9,\n",
       "         'occasion': 5,\n",
       "         'bitter': 17,\n",
       "         'ministers': 21,\n",
       "         'entering': 16,\n",
       "         'rage': 7,\n",
       "         'cities\\\\/abc': 7,\n",
       "         'supplies': 56,\n",
       "         'lets': 6,\n",
       "         'grounds': 20,\n",
       "         'downturn': 36,\n",
       "         'perfectly': 15,\n",
       "         'debentures': 80,\n",
       "         'nbi': 13,\n",
       "         'oldest': 7,\n",
       "         'outlawed': 6,\n",
       "         'mainly': 54,\n",
       "         'unused': 7,\n",
       "         'as': 4833,\n",
       "         'candidates': 52,\n",
       "         'nora': 6,\n",
       "         'fairly': 50,\n",
       "         'lows': 14,\n",
       "         'across': 90,\n",
       "         'policies': 79,\n",
       "         'u.s.s.r.': 12,\n",
       "         'got': 183,\n",
       "         'settling': 16,\n",
       "         'mixte': 58,\n",
       "         'competent': 6,\n",
       "         'usage': 7,\n",
       "         'rival': 45,\n",
       "         'schools': 62,\n",
       "         'daly': 17,\n",
       "         'stabilizing': 12,\n",
       "         'fabric': 10,\n",
       "         'computing': 19,\n",
       "         'limits': 41,\n",
       "         'eroding': 6,\n",
       "         'editions': 6,\n",
       "         'archrival': 8,\n",
       "         'impressive': 13,\n",
       "         'church': 50,\n",
       "         'three-month': 36,\n",
       "         'nowhere': 6,\n",
       "         'leap': 10,\n",
       "         'kid': 9,\n",
       "         'sit': 29,\n",
       "         'patience': 5,\n",
       "         'border': 16,\n",
       "         'sought': 72,\n",
       "         'diplomats': 8,\n",
       "         'plc': 114,\n",
       "         'petrochemical': 20,\n",
       "         'everybody': 35,\n",
       "         'baby': 24,\n",
       "         'dale': 9,\n",
       "         'however': 416,\n",
       "         'referring': 28,\n",
       "         'personnel': 46,\n",
       "         'psychiatric': 8,\n",
       "         'genentech': 14,\n",
       "         'abuses': 7,\n",
       "         'biggest': 146,\n",
       "         'carr': 5,\n",
       "         'participating': 18,\n",
       "         'meals': 7,\n",
       "         'cosmetics': 34,\n",
       "         'line': 228,\n",
       "         'reflecting': 80,\n",
       "         'rapidly': 43,\n",
       "         'repurchase': 22,\n",
       "         'newly': 44,\n",
       "         'rational': 10,\n",
       "         'idea': 92,\n",
       "         'hotel': 59,\n",
       "         'ill.': 26,\n",
       "         'funded': 24,\n",
       "         'textile': 18,\n",
       "         'all': 1227,\n",
       "         'climb': 18,\n",
       "         'wore': 8,\n",
       "         'expressed': 44,\n",
       "         'traub': 6,\n",
       "         'defaulted': 6,\n",
       "         'jan': 3,\n",
       "         'influx': 4,\n",
       "         'hotels': 19,\n",
       "         'craven': 6,\n",
       "         'economists': 95,\n",
       "         'studying': 24,\n",
       "         'hurricane': 93,\n",
       "         'fast-growing': 10,\n",
       "         'nicaraguan': 19,\n",
       "         'sanford': 17,\n",
       "         'jon': 13,\n",
       "         'jurors': 21,\n",
       "         'maine': 18,\n",
       "         'concept': 32,\n",
       "         'subsidized': 11,\n",
       "         'amoco': 21,\n",
       "         'surprised': 37,\n",
       "         'traditionally': 25,\n",
       "         'd': 12,\n",
       "         'gum': 9,\n",
       "         'programs': 192,\n",
       "         'refunding': 15,\n",
       "         'retrieve': 5,\n",
       "         'financings': 7,\n",
       "         'recognized': 20,\n",
       "         'privately': 41,\n",
       "         'anti-nuclear': 5,\n",
       "         'battered': 12,\n",
       "         'philip': 60,\n",
       "         'cherokee': 6,\n",
       "         '13th': 11,\n",
       "         'preamble': 8,\n",
       "         'inevitably': 10,\n",
       "         'deadlines': 7,\n",
       "         'societe': 17,\n",
       "         'parts': 116,\n",
       "         'ventures': 59,\n",
       "         'investigation': 77,\n",
       "         'airways': 32,\n",
       "         'schedule': 33,\n",
       "         'happens': 26,\n",
       "         'blast': 8,\n",
       "         'disrupt': 7,\n",
       "         'olympia': 9,\n",
       "         \"n't\": 3388,\n",
       "         'purchase': 219,\n",
       "         'calculate': 4,\n",
       "         'miles': 76,\n",
       "         'gnp': 26,\n",
       "         'himself': 103,\n",
       "         'mitsubishi': 63,\n",
       "         'eyes': 24,\n",
       "         'scaled': 9,\n",
       "         'nicaragua': 37,\n",
       "         'genes': 42,\n",
       "         'ridley': 9,\n",
       "         'jerry': 18,\n",
       "         'inquiries': 15,\n",
       "         'scaring': 6,\n",
       "         'east': 211,\n",
       "         'gone': 54,\n",
       "         'elimination': 8,\n",
       "         'committee': 281,\n",
       "         'external': 11,\n",
       "         'specifications': 6,\n",
       "         'seidman': 26,\n",
       "         'will': 3270,\n",
       "         'attract': 43,\n",
       "         'guerrilla': 8,\n",
       "         'seven-year': 10,\n",
       "         'require': 87,\n",
       "         'gains': 195,\n",
       "         'performance': 141,\n",
       "         'high-end': 7,\n",
       "         'reupke': 6,\n",
       "         'u': 10,\n",
       "         'holmes': 11,\n",
       "         'contra': 19,\n",
       "         'best': 182,\n",
       "         'doubled': 32,\n",
       "         'liquidation': 13,\n",
       "         'streamlining': 12,\n",
       "         'warned': 42,\n",
       "         '19th': 7,\n",
       "         'components': 21,\n",
       "         'reduce': 151,\n",
       "         'respectively': 23,\n",
       "         'digital': 104,\n",
       "         'democrat': 31,\n",
       "         'weak': 68,\n",
       "         'indicate': 44,\n",
       "         'political': 271,\n",
       "         'therefore': 21,\n",
       "         'manuel': 12,\n",
       "         'cross': 47,\n",
       "         'stance': 21,\n",
       "         'regard': 14,\n",
       "         'failures': 23,\n",
       "         'copyright': 17,\n",
       "         'fusion': 20,\n",
       "         'inherited': 8,\n",
       "         'classical': 5,\n",
       "         'reinforcing': 6,\n",
       "         'letting': 17,\n",
       "         'random': 23,\n",
       "         'dell': 15,\n",
       "         'macy': 11,\n",
       "         'bleeding': 6,\n",
       "         'adjusting': 9,\n",
       "         'prohibits': 8,\n",
       "         'ranks': 18,\n",
       "         'introduce': 36,\n",
       "         'earning': 16,\n",
       "         'appeal': 60,\n",
       "         'kansas': 19,\n",
       "         'galvanized': 11,\n",
       "         'jay': 14,\n",
       "         'offered': 220,\n",
       "         'curtailed': 6,\n",
       "         'employ': 8,\n",
       "         'vehicles': 65,\n",
       "         'neighborhood': 23,\n",
       "         'semiconductors': 10,\n",
       "         'cruz': 8,\n",
       "         'devised': 7,\n",
       "         'preparing': 20,\n",
       "         'sent': 109,\n",
       "         'doctor': 22,\n",
       "         'freed': 5,\n",
       "         'barber': 7,\n",
       "         'sufficient': 30,\n",
       "         'category': 27,\n",
       "         'dominant': 20,\n",
       "         'meaningful': 8,\n",
       "         'requiring': 31,\n",
       "         'hitachi': 18,\n",
       "         'oversight': 10,\n",
       "         'story': 76,\n",
       "         'namibia': 5,\n",
       "         'examples': 6,\n",
       "         'stemming': 18,\n",
       "         'mart': 12,\n",
       "         'amount': 201,\n",
       "         'ky.': 16,\n",
       "         'grim': 10,\n",
       "         'koch': 9,\n",
       "         'irving': 13,\n",
       "         'musical': 10,\n",
       "         'growth': 400,\n",
       "         'arabia': 13,\n",
       "         'couples': 17,\n",
       "         'diagnostic': 9,\n",
       "         'extensive': 24,\n",
       "         'money': 569,\n",
       "         'ethnic': 9,\n",
       "         'acquisition': 226,\n",
       "         'buyer': 49,\n",
       "         'investor': 216,\n",
       "         'upgraded': 6,\n",
       "         'tests': 62,\n",
       "         'terms': 211,\n",
       "         'highs': 19,\n",
       "         'kronor': 13,\n",
       "         'buddy': 8,\n",
       "         'younkers': 6,\n",
       "         'projects': 125,\n",
       "         'manic': 7,\n",
       "         'accurately': 8,\n",
       "         'wheat': 20,\n",
       "         'relationship': 52,\n",
       "         'paterson': 6,\n",
       "         'perspective': 25,\n",
       "         'commissions': 42,\n",
       "         'belongs': 9,\n",
       "         'jump': 45,\n",
       "         'format': 5,\n",
       "         'unit': 547,\n",
       "         'cites': 11,\n",
       "         'crises': 10,\n",
       "         'saudi': 23,\n",
       "         'write': 40,\n",
       "         'unconsolidated': 10,\n",
       "         'carat': 12,\n",
       "         'quotron': 16,\n",
       "         'subordinate': 8,\n",
       "         'entire': 72,\n",
       "         'capped': 28,\n",
       "         '12-month': 9,\n",
       "         'indianapolis': 15,\n",
       "         'dreyfus': 11,\n",
       "         'incident': 14,\n",
       "         'outsider': 5,\n",
       "         'principals': 9,\n",
       "         'mark': 102,\n",
       "         'fulfill': 9,\n",
       "         'revenue': 484,\n",
       "         'kate': 12,\n",
       "         'avoiding': 10,\n",
       "         'ferc': 1,\n",
       "         'relatively': 80,\n",
       "         'la': 48,\n",
       "         'pride': 15,\n",
       "         'invited': 16,\n",
       "         'likelihood': 12,\n",
       "         'let': 106,\n",
       "         'spent': 93,\n",
       "         'endangered': 6,\n",
       "         'procedures': 33,\n",
       "         'shipyard': 19,\n",
       "         'coal': 40,\n",
       "         'knowledgeable': 8,\n",
       "         'impact': 124,\n",
       "         'frightened': 6,\n",
       "         'constraints': 8,\n",
       "         'route': 12,\n",
       "         'depression': 13,\n",
       "         'berkeley': 21,\n",
       "         'quotes': 14,\n",
       "         'outcry': 15,\n",
       "         'marked': 22,\n",
       "         'profitability': 36,\n",
       "         'institutional': 84,\n",
       "         'critic': 11,\n",
       "         'masson': 12,\n",
       "         'station': 53,\n",
       "         'cheaper': 29,\n",
       "         'agricultural': 20,\n",
       "         'attendants': 16,\n",
       "         'municipal': 69,\n",
       "         'taught': 13,\n",
       "         'carol': 12,\n",
       "         'engines': 19,\n",
       "         'post-crash': 6,\n",
       "         'pcs': 16,\n",
       "         'sim': 1,\n",
       "         'await': 5,\n",
       "         'norwood': 7,\n",
       "         'doyle': 6,\n",
       "         'hedging': 10,\n",
       "         'sons': 28,\n",
       "         'pulp': 36,\n",
       "         'micro': 5,\n",
       "         'clients': 141,\n",
       "         'mack': 6,\n",
       "         'basically': 21,\n",
       "         'salary': 21,\n",
       "         'syrian': 6,\n",
       "         'obtained': 25,\n",
       "         'building': 223,\n",
       "         'policy': 281,\n",
       "         'connecting': 6,\n",
       "         'cold': 42,\n",
       "         'ryder': 10,\n",
       "         'permitted': 27,\n",
       "         'efficiently': 6,\n",
       "         'damages': 47,\n",
       "         'improvements': 24,\n",
       "         'herself': 11,\n",
       "         'shakespeare': 8,\n",
       "         'campbell': 35,\n",
       "         'displayed': 7,\n",
       "         'notification': 9,\n",
       "         'matthews': 6,\n",
       "         'paribas': 66,\n",
       "         'stunning': 7,\n",
       "         'sherwin': 8,\n",
       "         'cameras': 6,\n",
       "         'activities': 86,\n",
       "         'coors': 30,\n",
       "         'wathen': 18,\n",
       "         'atlanta': 38,\n",
       "         'low-income': 10,\n",
       "         'creatures': 7,\n",
       "         'fronts': 6,\n",
       "         'fray': 6,\n",
       "         'attend': 7,\n",
       "         'anti-abortion': 13,\n",
       "         'net': 651,\n",
       "         'restore': 27,\n",
       "         'lose': 68,\n",
       "         'co.': 939,\n",
       "         'emergency': 91,\n",
       "         'pulls': 7,\n",
       "         'objective': 11,\n",
       "         'code': 39,\n",
       "         'seem': 103,\n",
       "         'intelogic': 14,\n",
       "         'find': 160,\n",
       "         'elected': 85,\n",
       "         'appearing': 10,\n",
       "         'everywhere': 10,\n",
       "         'shuttle': 17,\n",
       "         'nevertheless': 30,\n",
       "         'little': 320,\n",
       "         'trapped': 6,\n",
       "         'etc': 4,\n",
       "         'changes': 213,\n",
       "         'builds': 10,\n",
       "         'rey': 31,\n",
       "         'estimating': 7,\n",
       "         'turn': 130,\n",
       "         'poughkeepsie': 6,\n",
       "         'limit': 103,\n",
       "         'wild': 30,\n",
       "         'costly': 44,\n",
       "         'admitted': 25,\n",
       "         'york': 961,\n",
       "         'unemployment': 39,\n",
       "         'ecological': 6,\n",
       "         'acting': 28,\n",
       "         'be': 3923,\n",
       "         'aug.': 48,\n",
       "         'murray': 15,\n",
       "         'jazz': 8,\n",
       "         'delicate': 7,\n",
       "         'vision': 18,\n",
       "         'buck': 10,\n",
       "         'needed': 120,\n",
       "         'model': 56,\n",
       "         'rare': 27,\n",
       "         'persistent': 15,\n",
       "         'flawed': 8,\n",
       "         'define': 5,\n",
       "         'expenditures': 20,\n",
       "         'upward': 21,\n",
       "         'sugar': 50,\n",
       "         'vicar': 7,\n",
       "         'hoelzer': 6,\n",
       "         'workers': 247,\n",
       "         'keep': 186,\n",
       "         'sweeping': 24,\n",
       "         'walt': 10,\n",
       "         'copper': 70,\n",
       "         'wrongdoing': 17,\n",
       "         'retreated': 9,\n",
       "         'treatment': 76,\n",
       "         'trendy': 5,\n",
       "         'china': 170,\n",
       "         'chasing': 6,\n",
       "         'pro-choice': 20,\n",
       "         'strict': 17,\n",
       "         'bidders': 25,\n",
       "         'basir': 6,\n",
       "         'prebon': 11,\n",
       "         'lagging': 11,\n",
       "         'australian': 57,\n",
       "         'projecting': 7,\n",
       "         'summit': 18,\n",
       "         'harmony': 6,\n",
       "         'typical': 35,\n",
       "         'surfaced': 14,\n",
       "         'meant': 31,\n",
       "         'auto': 176,\n",
       "         'peddling': 8,\n",
       "         'adoption': 10,\n",
       "         'los': 170,\n",
       "         'omni': 6,\n",
       "         'bit': 53,\n",
       "         'soften': 6,\n",
       "         'heightened': 10,\n",
       "         'chevron': 22,\n",
       "         'values': 60,\n",
       "         'america': 202,\n",
       "         'trends': 34,\n",
       "         'healthy': 56,\n",
       "         'prepare': 20,\n",
       "         'pop': 24,\n",
       "         'charitable': 10,\n",
       "         'lasts': 6,\n",
       "         'auctions': 11,\n",
       "         'hatch': 6,\n",
       "         'normally': 44,\n",
       "         'tendered': 17,\n",
       "         'market-makers': 10,\n",
       "         'bridges': 23,\n",
       "         'formerly': 39,\n",
       "         'memory': 37,\n",
       "         'repairs': 15,\n",
       "         'containers': 35,\n",
       "         'text': 15,\n",
       "         'subscribers': 24,\n",
       "         'accomplish': 10,\n",
       "         'officer': 344,\n",
       "         'swift': 8,\n",
       "         'neck': 11,\n",
       "         'protects': 7,\n",
       "         'one-hour': 10,\n",
       "         'awful': 10,\n",
       "         'pinkerton': 28,\n",
       "         'equipped': 9,\n",
       "         'supposed': 38,\n",
       "         'informed': 20,\n",
       "         'surprise': 35,\n",
       "         'bond': 364,\n",
       "         'bond-equivalent': 5,\n",
       "         'beneficial': 17,\n",
       "         'distinguished': 6,\n",
       "         'door': 47,\n",
       "         'equity': 174,\n",
       "         'issuing': 14,\n",
       "         'fiscal': 253,\n",
       "         'twelve': 5,\n",
       "         'toronto': 46,\n",
       "         'fleeting': 5,\n",
       "         'gene': 63,\n",
       "         'behalf': 32,\n",
       "         'averages': 11,\n",
       "         'contest': 20,\n",
       "         'mountain-bike': 8,\n",
       "         'rtc': 16,\n",
       "         'tool': 22,\n",
       "         '&': 1069,\n",
       "         'communication': 17,\n",
       "         'five-year': 34,\n",
       "         'francis': 9,\n",
       "         'repay': 18,\n",
       "         'noise': 7,\n",
       "         'rebounded': 18,\n",
       "         'shrank': 6,\n",
       "         'benchmark': 55,\n",
       "         'main': 96,\n",
       "         'sky': 10,\n",
       "         'literature': 9,\n",
       "         'martin': 39,\n",
       "         'specific': 86,\n",
       "         'head': 170,\n",
       "         'stepping': 11,\n",
       "         'crop': 41,\n",
       "         'agree': 54,\n",
       "         'entrepreneur': 17,\n",
       "         'driven': 21,\n",
       "         'mafia': 8,\n",
       "         'relieve': 9,\n",
       "         'discover': 11,\n",
       "         'high-risk': 26,\n",
       "         'could': 1136,\n",
       "         'freedoms': 8,\n",
       "         'mainstay': 7,\n",
       "         'passive': 11,\n",
       "         'grade': 8,\n",
       "         'bargains': 7,\n",
       "         'hoped': 31,\n",
       "         'mention': 19,\n",
       "         'illustrates': 11,\n",
       "         'supplement': 6,\n",
       "         'coast': 55,\n",
       "         'rule': 92,\n",
       "         'rows': 6,\n",
       "         'rudolph': 8,\n",
       "         'exploit': 10,\n",
       "         'obtain': 45,\n",
       "         'resisted': 12,\n",
       "         'opec': 36,\n",
       "         'weirton': 4,\n",
       "         'epicenter': 12,\n",
       "         'top': 208,\n",
       "         'taxpayers': 44,\n",
       "         'replacement': 24,\n",
       "         'weeks': 250,\n",
       "         'capitalist': 8,\n",
       "         'hung': 6,\n",
       "         'cupertino': 8,\n",
       "         'engine': 33,\n",
       "         'full': 171,\n",
       "         'taxes': 114,\n",
       "         'streets': 26,\n",
       "         'percent': 16,\n",
       "         'wo': 252,\n",
       "         'shippers': 15,\n",
       "         'delaying': 8,\n",
       "         'pose': 7,\n",
       "         'none': 52,\n",
       "         'sector': 101,\n",
       "         'germany': 154,\n",
       "         'tramp': 6,\n",
       "         'cast': 29,\n",
       "         'numbers': 63,\n",
       "         'facilities': 88,\n",
       "         'fourth': 120,\n",
       "         'who': 1695,\n",
       "         'strikes': 19,\n",
       "         'sales': 1126,\n",
       "         'diverted': 11,\n",
       "         'jefferies': 8,\n",
       "         'ltv': 34,\n",
       "         'blue-chip': 33,\n",
       "         'maxwell': 37,\n",
       "         'guild': 10,\n",
       "         'lancaster': 5,\n",
       "         'golf': 20,\n",
       "         'unity': 7,\n",
       "         'hurting': 12,\n",
       "         'touting': 6,\n",
       "         'enserch': 6,\n",
       "         'policyholders': 8,\n",
       "         'deliveries': 7,\n",
       "         'world-wide': 74,\n",
       "         'population': 37,\n",
       "         'herald': 18,\n",
       "         'goodwill': 7,\n",
       "         'dozen': 60,\n",
       "         'crisis': 47,\n",
       "         'industrial': 243,\n",
       "         'paying': 95,\n",
       "         'eagle': 25,\n",
       "         'texans': 12,\n",
       "         'treasurys': 20,\n",
       "         'illinois': 35,\n",
       "         'yes': 18,\n",
       "         'prefer': 26,\n",
       "         'bankruptcy-court': 5,\n",
       "         'mill': 24,\n",
       "         'drabinsky': 18,\n",
       "         'backed': 79,\n",
       "         'headquarters': 87,\n",
       "         'gambling': 14,\n",
       "         'governments': 40,\n",
       "         'advisory': 21,\n",
       "         'nsc': 16,\n",
       "         'previous': 192,\n",
       "         'lynch': 130,\n",
       "         'iowa': 26,\n",
       "         'casting': 13,\n",
       "         'discussions': 50,\n",
       "         'greece': 11,\n",
       "         'appealing': 8,\n",
       "         'reported': 430,\n",
       "         'helping': 46,\n",
       "         'reopen': 16,\n",
       "         'gun': 7,\n",
       "         'endless': 6,\n",
       "         'unix': 8,\n",
       "         'slowdown': 61,\n",
       "         'advancing': 18,\n",
       "         'exposures': 7,\n",
       "         'rep': 5,\n",
       "         'customer': 76,\n",
       "         'expanded': 38,\n",
       "         'bars': 8,\n",
       "         'lighter': 8,\n",
       "         'beneficiaries': 16,\n",
       "         'plug': 7,\n",
       "         'incidents': 7,\n",
       "         'invests': 5,\n",
       "         'vermont': 15,\n",
       "         'edt': 16,\n",
       "         'colombia': 13,\n",
       "         'brothers': 109,\n",
       "         'privatized': 6,\n",
       "         'diego': 18,\n",
       "         'loser': 6,\n",
       "         'rush': 15,\n",
       "         'closely': 97,\n",
       "         'sorrell': 8,\n",
       "         'massachusetts': 36,\n",
       "         'merger': 105,\n",
       "         'factories': 25,\n",
       "         'vs.': 34,\n",
       "         'enforcement': 36,\n",
       "         'prompting': 13,\n",
       "         'word': 47,\n",
       "         'revenues': 36,\n",
       "         'profits': 147,\n",
       "         'scuttle': 10,\n",
       "         'considerably': 23,\n",
       "         'modern': 39,\n",
       "         'turmoil': 33,\n",
       "         'jefferson': 8,\n",
       "         '1\\\\/2-year': 8,\n",
       "         'reaction': 46,\n",
       "         'handful': 30,\n",
       "         'larger': 80,\n",
       "         'civilian': 6,\n",
       "         'look': 151,\n",
       "         'corn': 65,\n",
       "         'depositary': 17,\n",
       "         'enables': 6,\n",
       "         'altogether': 17,\n",
       "         'unanimously': 11,\n",
       "         'afterward': 10,\n",
       "         'calif.': 128,\n",
       "         'guterman': 1,\n",
       "         'justin': 2,\n",
       "         'scoring': 13,\n",
       "         'enforcers': 7,\n",
       "         'horses': 9,\n",
       "         \"'ve\": 188,\n",
       "         'accounted': 33,\n",
       "         'retains': 13,\n",
       "         'competes': 7,\n",
       "         'chuck': 6,\n",
       "         'color': 34,\n",
       "         'physicians': 7,\n",
       "         'meet': 122,\n",
       "         'lawn': 11,\n",
       "         'nicholas': 21,\n",
       "         'least': 341,\n",
       "         'loses': 9,\n",
       "         'settlement': 119,\n",
       "         'matched': 18,\n",
       "         'would-be': 13,\n",
       "         'influenced': 14,\n",
       "         'neil': 9,\n",
       "         'hardware': 23,\n",
       "         'dragged': 8,\n",
       "         'injunction': 21,\n",
       "         'sheet': 29,\n",
       "         'paris': 59,\n",
       "         'intimate': 12,\n",
       "         'plus': 59,\n",
       "         'sequester': 7,\n",
       "         'simpson': 8,\n",
       "         'chlorofluorocarbons': 5,\n",
       "         'satellite': 24,\n",
       "         'cuba': 16,\n",
       "         'proliferation': 7,\n",
       "         'statistics': 51,\n",
       "         'shortage': 21,\n",
       "         'confirming': 6,\n",
       "         'column': 17,\n",
       "         'highland': 9,\n",
       "         'our': 416,\n",
       "         'row': 23,\n",
       "         'bell': 80,\n",
       "         'drafted': 10,\n",
       "         'amex': 24,\n",
       "         'economist': 88,\n",
       "         'entrepreneurial': 5,\n",
       "         'brands': 78,\n",
       "         'succeeded': 30,\n",
       "         'lauder': 10,\n",
       "         'rubber': 13,\n",
       "         'ginnie': 14,\n",
       "         'madrid': 6,\n",
       "         'stevens': 15,\n",
       "         'fbi': 26,\n",
       "         'state': 644,\n",
       "         'hard': 152,\n",
       "         'surveys': 13,\n",
       "         'contel': 15,\n",
       "         'expect': 209,\n",
       "         'dunn': 6,\n",
       "         'taking': 130,\n",
       "         'tissue': 14,\n",
       "         'periods': 19,\n",
       "         'utilization': 13,\n",
       "         'help': 313,\n",
       "         'sheep': 6,\n",
       "         'ncaa': 6,\n",
       "         'provisions': 81,\n",
       "         'inco': 14,\n",
       "         'morgan': 121,\n",
       "         'copy': 29,\n",
       "         'proposes': 11,\n",
       "         'functions': 17,\n",
       "         'furthermore': 10,\n",
       "         'apartment': 21,\n",
       "         'tenn.': 13,\n",
       "         'current': 371,\n",
       "         'roebuck': 10,\n",
       "         'ireland': 10,\n",
       "         'saw': 70,\n",
       "         'bowes': 7,\n",
       "         'tough': 74,\n",
       "         'innovation': 11,\n",
       "         'synthetic': 11,\n",
       "         'calculated': 28,\n",
       "         'unsuccessful': 19,\n",
       "         'bill': 376,\n",
       "         'occur': 26,\n",
       "         'mushrooms': 8,\n",
       "         'size': 101,\n",
       "         'leventhal': 7,\n",
       "         'limited': 137,\n",
       "         'eggs': 32,\n",
       "         'ticket': 26,\n",
       "         'saturday': 47,\n",
       "         'alike': 11,\n",
       "         'segments': 24,\n",
       "         'again': 194,\n",
       "         'bullion': 7,\n",
       "         'tight': 28,\n",
       "         'authorization': 12,\n",
       "         'merely': 38,\n",
       "         'unisys': 25,\n",
       "         'cat': 7,\n",
       "         'lid': 6,\n",
       "         'single-a-3': 10,\n",
       "         'including': 445,\n",
       "         'advertisers': 59,\n",
       "         'folk': 6,\n",
       "         'trimmed': 15,\n",
       "         'leonard': 9,\n",
       "         'george': 106,\n",
       "         'broaden': 6,\n",
       "         'establish': 36,\n",
       "         'burt': 8,\n",
       "         'transaction': 191,\n",
       "         'now': 875,\n",
       "         'axa': 8,\n",
       "         'exactly': 34,\n",
       "         'redeemed': 12,\n",
       "         'analyzing': 6,\n",
       "         'interested': 75,\n",
       "         'syndicates': 8,\n",
       "         'trigger': 13,\n",
       "         'imo': 6,\n",
       "         'prints': 4,\n",
       "         'at': 4894,\n",
       "         'farrell': 3,\n",
       "         'apiece': 17,\n",
       "         'dishonesty': 6,\n",
       "         'disruption': 11,\n",
       "         'nobel': 10,\n",
       "         'average': 467,\n",
       "         'urge': 17,\n",
       "         'citizens': 39,\n",
       "         'attributes': 8,\n",
       "         'persian': 5,\n",
       "         'withheld': 9,\n",
       "         'energy': 140,\n",
       "         'periodic': 7,\n",
       "         'virtually': 57,\n",
       "         'wife': 50,\n",
       "         'greenville': 16,\n",
       "         'his': 1852,\n",
       "         'occurs': 12,\n",
       "         'birds': 6,\n",
       "         'salt': 10,\n",
       "         'weakness': 55,\n",
       "         'bros.': 8,\n",
       "         'bureaucrat': 6,\n",
       "         'troublesome': 7,\n",
       "         'depend': 11,\n",
       "         'longstanding': 16,\n",
       "         'iran-contra': 16,\n",
       "         'commerciale': 7,\n",
       "         'amgen': 8,\n",
       "         'raising': 73,\n",
       "         'skase': 15,\n",
       "         'loosen': 7,\n",
       "         'september': 332,\n",
       "         'offset': 88,\n",
       "         'understood': 16,\n",
       "         'blues': 5,\n",
       "         'georgia-pacific': 61,\n",
       "         'yeast': 9,\n",
       "         'curbs': 8,\n",
       "         'frustration': 10,\n",
       "         'alternatively': 6,\n",
       "         'regulated': 13,\n",
       "         'gte': 16,\n",
       "         'producing': 53,\n",
       "         'bread': 8,\n",
       "         'switched': 16,\n",
       "         'directs': 9,\n",
       "         'wherever': 5,\n",
       "         'count': 21,\n",
       "         'boone': 6,\n",
       "         'prosecutorial': 7,\n",
       "         'guest': 8,\n",
       "         'cry': 6,\n",
       "         'merchant': 36,\n",
       "         'squibb': 15,\n",
       "         'insure': 8,\n",
       "         'topple': 6,\n",
       "         'smiling': 5,\n",
       "         'phillips': 53,\n",
       "         'first-time': 5,\n",
       "         'prompt': 11,\n",
       "         'drops': 13,\n",
       "         'diminished': 16,\n",
       "         'refrigerators': 5,\n",
       "         'ties': 43,\n",
       "         'belong': 11,\n",
       "         'recruiting': 12,\n",
       "         'warrants': 48,\n",
       "         'approaches': 12,\n",
       "         'scientist': 16,\n",
       "         'motel': 6,\n",
       "         'downey': 6,\n",
       "         'giant': 98,\n",
       "         'additions': 6,\n",
       "         'joan': 7,\n",
       "         'prudential-bache': 34,\n",
       "         'proper': 18,\n",
       "         'provided': 80,\n",
       "         'supply': 117,\n",
       "         'isolated': 11,\n",
       "         'wash.': 10,\n",
       "         'wright': 23,\n",
       "         'cutler': 6,\n",
       "         'medicaid': 12,\n",
       "         'dealers': 163,\n",
       "         'infrastructure': 12,\n",
       "         'study': 116,\n",
       "         'simon': 8,\n",
       "         'nancy': 10,\n",
       "         'brought': 94,\n",
       "         'difficulties': 29,\n",
       "         'special': 166,\n",
       "         'jordan': 9,\n",
       "         'palo': 21,\n",
       "         'fisher': 9,\n",
       "         'transcanada': 12,\n",
       "         'office': 352,\n",
       "         'dependent': 9,\n",
       "         'collective': 10,\n",
       "         'waertsilae': 29,\n",
       "         'disproportionate': 10,\n",
       "         'insist': 35,\n",
       "         'caught': 38,\n",
       "         'given': 149,\n",
       "         'bradley': 11,\n",
       "         'championship': 6,\n",
       "         'differences': 41,\n",
       "         'sen': 6,\n",
       "         'canceled': 25,\n",
       "         'liquidity': 54,\n",
       "         'tvs': 22,\n",
       "         'affairs': 59,\n",
       "         'cooperation': 40,\n",
       "         'introduction': 22,\n",
       "         'coats': 8,\n",
       "         'faltered': 6,\n",
       "         'rapid': 35,\n",
       "         'demonstrators': 12,\n",
       "         'catastrophic': 15,\n",
       "         'cap': 23,\n",
       "         'sluggish': 35,\n",
       "         'alex': 20,\n",
       "         'friend': 32,\n",
       "         'arafat': 12,\n",
       "         'manpower': 4,\n",
       "         'celebrating': 5,\n",
       "         'vowed': 13,\n",
       "         'madison': 17,\n",
       "         'per-share': 53,\n",
       "         'novel': 24,\n",
       "         'merkur': 19,\n",
       "         'bear': 62,\n",
       "         'any': 827,\n",
       "         'corporate': 295,\n",
       "         'sorts': 11,\n",
       "         'recommending': 9,\n",
       "         'fresh': 35,\n",
       "         'takes': 84,\n",
       "         'careful': 16,\n",
       "         'arco': 10,\n",
       "         'revolving': 5,\n",
       "         'totally': 28,\n",
       "         'federated': 14,\n",
       "         'should': 457,\n",
       "         'shut': 30,\n",
       "         'risky': 23,\n",
       "         'lacked': 11,\n",
       "         ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按词频对单词进行排序\n",
    "sorted_word_to_cnt = sorted(counter.items(),\n",
    "                            key=itemgetter(1),\n",
    "                            reverse=True)\n",
    "sorted_words = [x[0] for x in sorted_word_to_cnt] #词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#将句子结束符\"<eos>\"加入词汇表\n",
    "sorted_words = [\"<eos>\"] + sorted_words\n",
    "\n",
    "with codecs.open(VOCAB_OUTPUT, \"w\", \"utf-8\") as file_output:  #将词汇表保存在一个文件中\n",
    "    for word in sorted_words:\n",
    "        file_output.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练文件、测试文件根据词汇文件转换为单词编号\n",
    "VOCAB = \"ptb.vocab\"\n",
    "OUTPUT_DATA = \"ptb.train\"\n",
    "\n",
    "#读取词汇表\n",
    "with codecs.open(VOCAB, \"r\", \"utf-8\") as f_vocab:\n",
    "    vocab = [w.strip() for w in f_vocab.readlines()]\n",
    "    \n",
    "word_to_id = {k: v for (k, v) in zip(vocab, range(len(vocab)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果出现了被删除的低频词，则替换为\"<unk>\"\n",
    "def get_id(word):\n",
    "    return word_to_id[word] if word in word_to_id else word_to_id[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DATA = \"ptb.test\"\n",
    "RAW_DATA = \"../dataset/simple-examples/data/ptb.test.txt\"\n",
    "fin = codecs.open(RAW_DATA, \"r\", \"utf-8\")\n",
    "fout = codecs.open(OUTPUT_DATA, \"w\", \"utf-8\")\n",
    "for line in fin:\n",
    "    words = line.strip().split() + [\"<eos>\"] #读取单词并添加<eos>结束符\n",
    "    out_line = ' '.join([str(get_id(w)) for w in words]) + '\\n'\n",
    "    fout.write(out_line)\n",
    "    \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 9994 9985 9973 9987 9992 9983 9979 9977 9970 9989 9986 9991 9974 9993 9997 9988 9972 9980 9975 9996 9981 9976 9998 0\n",
      "\n",
      "9273 2 3 72 393 33 2123 1 146 19 6 9143 275 408 3 0\n",
      "\n",
      "23 2 13 142 4 2 5340 1 3136 1591 96 0\n",
      "\n",
      "7331 2 3 72 393 8 337 142 4 2500 659 2191 955 24 522 6 9143 275 4 39 303 438 3675 0\n",
      "\n",
      "6 944 4 3214 498 263 5 137 5969 4227 6135 30 995 6 240 757 4 1014 2770 212 6 96 4 427 4097 5 14 45 55 3 72 195 1236 220 0\n",
      "\n",
      "1 3214 7519 2 13 4052 2 498 14 6915 1 2 22 113 2674 8376 5 14 2503 5245 10 463 52 3029 466 1236 15 0\n",
      "\n",
      "2 80 1 167 4 35 2613 2 65 10 559 5969 3631 1891 665 2 7 27 2 4227 6135 7 3 0\n",
      "\n",
      "367 1960 3205 46 220 45 55 6 40 195 1 467 342 1298 7 325 9 35 1503 919 4 3193 6 8658 371 5 1156 35 1415 5 1 433 0\n",
      "\n",
      "6 2 2 15 39 13 31 393 1364 0\n",
      "\n",
      "64 277 1922 43 72 195 157 1451 2371 4 3214 718 106 5754 1306 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(\"ptb.train\", \"r\", \"utf-8\") as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        print(line)\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132 93 358 5 329 51 9591 6 326 2490 5 1 661 384 0\n",
      "\n",
      "2 2 2937 2195 9 382 1071 2334 89 99 843 198 2 11 1 3396 1126 7 3 72 20 211 346 36 258 2 2 0\n",
      "\n",
      "75 421 195 3911 4 249 1805 2 579 3528 892 2420 6 3 296 11 2739 16 1185 2 250 0\n",
      "\n",
      "8 1 35 9917 3715 463 710 2992 2039 3911 135 6051 11 494 5967 16 1 130 272 9 463 0\n",
      "\n",
      "9959 731 503 30 640 6 35 6715 7 2 8 759 9958 26 6601 5 6333 1 6455 0\n",
      "\n",
      "1414 3911 93 1553 2 22 1 503 8 2 1 361 0\n",
      "\n",
      "29 383 99 9958 26 7367 10 3911 56 26 3277 9234 52 6 879 4 323 93 335 118 51 2 350 2 8 1335 2 0\n",
      "\n",
      "64 578 58 508 6 580 4 103 7 641 747 1900 5 661 359 108 44 5327 5968 71 4 791 9959 41 7464 503 11 179 2195 1257 8 1805 9 579 1496 0\n",
      "\n",
      "22 1 9805 4 1 759 47 144 171 1381 13 735 11 6 228 5 188 3911 45 9637 0\n",
      "\n",
      "54 4 1 818 1123 1 2426 269 4 3 1621 13 791 9959 6 795 817 4 2187 140 1017 95 8 140 731 82 3078 570 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(\"ptb.valid\", \"r\", \"utf-8\") as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        print(line)\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 14 24 32 753 382 0\n",
      "\n",
      "29 120 1 35 91 60 111 143 32 616 3205 282 19 1 447 458 437 196 1626 3 394 90 4 14 7 1 1113 1465 14 3163 1852 5 1335 39 1079 4 7223 0\n",
      "\n",
      "57 2192 4914 3858 78 1 522 3 1034 777 51 74 898 278 117 2274 5 4102 1 399 3834 7 179 149 8 287 0\n",
      "\n",
      "1 3 60 2579 365 16 1 129 146 1023 1 847 8 2888 4 69 3144 56 46 3043 78 1 3 1034 498 554 79 32 2432 1 399 842 0\n",
      "\n",
      "129 145 248 2064 5 1204 52 5 1 5462 5 414 1 8748 1023 278 17 362 129 4233 4 60 278 117 0\n",
      "\n",
      "773 399 4 738 82 886 9 3992 216 287 7 482 2 2808 149 3026 0\n",
      "\n",
      "915 129 146 149 407 3037 3622 6320 3965 180 7839 1669 1982 8 687 5706 96 1898 77 8 574 5698 0\n",
      "\n",
      "1 2 30 293 2362 0\n",
      "\n",
      "1 620 47 24 2 0\n",
      "\n",
      "498 554 1 2184 46 63 585 5 2432 1 6484 16 1 1023 4 1 35 91 60 111 15 2991 2 444 231 71 18 2 125 584 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(\"ptb.test\", \"r\", \"utf-8\") as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        print(line)\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTB数据的batching方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"ptb.train\"\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "TRAIN_NUM_STEP = 35\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        #将整个文档读进一个长字符串\n",
    "        id_string = ' '.join([line.strip() for line in fin.readlines()])\n",
    "    id_list = [int(w) for w in id_string.split()]\n",
    "    return id_list\n",
    "\n",
    "def make_batches(id_list, batch_size, num_step):\n",
    "    #计算总的batch数量 每个batch包含的单词数量是batch_size * num_step\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step)\n",
    "    \n",
    "    #将数据整理成一个维度为[batch_size, num_batches * num_step]的二维数组\n",
    "    data = np.array(id_list[: num_batches * batch_size * num_step])\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step])\n",
    "    #沿着第二个维度将数据切分成num_batches个batch,存入一个数组\n",
    "    data_batches = np.split(data, num_batches, axis=1)\n",
    "    \n",
    "    #重复上述操作，但是每个位置向右移动一位，这里得到的是RNN每一步输出所需要的下一个单词\n",
    "    label = np.array(id_list[1: num_batches * batch_size * num_step + 1])\n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)\n",
    "    \n",
    "    #返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵\n",
    "    return list(zip(data_batches, label_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = make_batches(read_data(TRAIN_DATA),\n",
    "                             TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[9999, 9994, 9985, 9973, 9987, 9992, 9983, 9979, 9977, 9970, 9989,\n",
       "          9986, 9991, 9974, 9993, 9997, 9988, 9972, 9980, 9975, 9996, 9981,\n",
       "          9976, 9998,    0, 9273,    2,    3,   72,  393,   33, 2123,    1,\n",
       "           146,   19],\n",
       "         [  13, 1511,   18, 1441,    1,  846,  234,    1, 1380,    5, 1281,\n",
       "             7, 1643, 1082, 3870,   17,  380, 1352,    4,  207,    0,    1,\n",
       "          2610,    4,    1,  261,   13,    5,  335,    1,    2,   16,  764,\n",
       "          1502,   10],\n",
       "         [   1, 1122,  644,   46,   20,    2, 1060,   82, 1092,  473,    6,\n",
       "          1916,    7,    2,    2,    8, 7588,   80,    6,    2, 2130,    7,\n",
       "          1933,    0, 5717,   82, 9028,  559,  549,    2,   22, 8662,    8,\n",
       "           537,    2],\n",
       "         [ 387,   14,   93,   25, 1019,    5,  254,  170,   10,  207,    0,\n",
       "            54, 1453, 1260,   22, 1661,   15,    1,  468,   42,   45,   55,\n",
       "          1846,    1,   37,    9,  207,    4,  513,   12,    3,   48,    0,\n",
       "            14,   59],\n",
       "         [  55,    3,  394,   69,  123,    0,  271,  112,  610,    5, 3403,\n",
       "           206,    7, 3369,    4,   45,  309, 1660,    6, 3333,  354,    0,\n",
       "           367,    1,  332,  119,  745,  174,   90,  138, 2407,    1, 1243,\n",
       "             7,  820],\n",
       "         [ 133,   53, 1525,  159,  727,   10,   23, 1384,    9,  217, 1290,\n",
       "            42,   34,    2, 1689,   16,  770,    9,  196,  907,    8,    1,\n",
       "           310,    4, 1644,    0,   38,  368,    1, 2435,  158,    4,  502,\n",
       "             8,  159],\n",
       "         [ 342,  120,   63,  309,   26,   32, 2683, 2703,  702,   18,  295,\n",
       "            63,  262, 2703,    0,    8,   57,  108,   87,  316,   10, 3064,\n",
       "            84, 7445,   13,   32, 8643,    4, 1023,  309,   41,  498, 6318,\n",
       "          7700,    5],\n",
       "         [   1,    2, 2185, 6738,    0,    1, 2185, 3531,   74, 3164,    1,\n",
       "             2,    3,   16,    6,    2, 8466,  170,    2,    2, 3574,    2,\n",
       "            23, 2661, 2552,   22, 8273,  325,    0,    1, 2185,  592,    2,\n",
       "             7,    1],\n",
       "         [ 286, 1048,  902,    1,  297,    4,  108,   56,   79, 4517,   11,\n",
       "            27,  992,    8, 5011,    2, 3483,   51,    2,  241,    0, 5011,\n",
       "           635,  294,   34, 2406,    5,   43,   12,    3,   48,   20,   12,\n",
       "             3,   48],\n",
       "         [   4, 2638, 4075,  130, 1204,   38,   15,    0,   11,    1,    3,\n",
       "          1162, 1394,    1, 9139, 9034,   13, 5331,    6,  391,  377,    7,\n",
       "             1,  771,    2,    8, 3073, 1162,  696,   41,   79, 1048,  244,\n",
       "             1,  270],\n",
       "         [2202, 4562,    4,  619,  184,   18,   47,   36,    2,  172,    0,\n",
       "           120,    1, 2202,   24,    7,  721,  131,  270,   42,  702,   25,\n",
       "           488,   17,    1, 2202,   36,  284,    5,    1,  500, 1696,    0,\n",
       "            39,  156],\n",
       "         [   4,  957, 8326, 2395,    0,   54, 3503,   24,    2,   84,    8,\n",
       "            27, 6743,    2,    0,  665,    1,   35, 3167, 1255,   17,    2,\n",
       "            65,  341,    1,    2, 1274, 2091,    1, 1474, 2205,  159, 2180,\n",
       "             4, 3256],\n",
       "         [ 208,   16,    1, 2164, 1051,  532,    4,   71,    4,    1, 1404,\n",
       "           302,    8,  151,  154,    4,   27,  196,  222,    0,   28,   50,\n",
       "            58,   71,    4,    1,  196,  222,    0,   23, 2605,   59,   50,\n",
       "             2, 2281],\n",
       "         [   0,    5,  881,    5, 3120,   45, 3023,   28,   30,  556,    2,\n",
       "          3281,  112,   20,   12,    3,    5,   12,    3,    0,   29,    1,\n",
       "           187, 1446,  725, 1119,  270,    7,    1, 2317,   30,   58,    5,\n",
       "           629,  112],\n",
       "         [ 997,    5, 2300,   43,    3,    3, 1141,  130,  855,    0, 3186,\n",
       "          1422,   80,   15,   14,  527,    6, 4568,  684,   22,    1,  125,\n",
       "             8,  111,  473,   11,    6,  273,  356,    4,  363,   21,  246,\n",
       "            70,    0],\n",
       "         [  57,  164, 1880,  229,   10,    1, 2294,   30, 4708,    7,  310,\n",
       "            19,   31, 4904,   76,  654,    8,   61,  189,   89,   25,  263,\n",
       "             5, 5367,  692, 1370,    0,    2,   65,  660,    1,  129,  146,\n",
       "          1399,    4],\n",
       "         [  11, 2438,   43,  690,    1,  225,   33,   59,  932, 2438,   10,\n",
       "          4068,    2,  196,   85,  754,   36,  129, 2754,   23,    2,   44,\n",
       "             0, 1211,  733,   80,    9,  564,   89, 6686,    1,  310,    4,\n",
       "            31, 1049],\n",
       "         [   0,   74,  285,   41,  372, 1662,  132,  301,  483,  568,  827,\n",
       "            15,   14,   42,  268,    1, 1334,    5, 2265,  581, 1585, 2656,\n",
       "             7,    1, 4479,    8, 1739,    0,   22,   10, 2274,    7, 1722,\n",
       "           164,   15],\n",
       "         [ 291,    3,   38,   46, 4429,  382, 3238,    0,    1,  357, 2888,\n",
       "            27,  337,    2,    5, 3534, 7848,    8,  450,    5, 5323,  594,\n",
       "            10,  215,  357,    7,    1, 2588, 2569,    5, 1485,   27,  194,\n",
       "          2092,    0],\n",
       "         [1301,  745,   20,    1,   12,    3,   21,    7,    1,  334,  109,\n",
       "             0,    8,   28, 1309,   10,   45, 1313, 1469,   13,  169,    7,\n",
       "             1,  130, 1552,    4, 2295,    0,    1,   37,  368,   27,  500,\n",
       "           253,  716]]),\n",
       "  array([[9994, 9985, 9973, 9987, 9992, 9983, 9979, 9977, 9970, 9989, 9986,\n",
       "          9991, 9974, 9993, 9997, 9988, 9972, 9980, 9975, 9996, 9981, 9976,\n",
       "          9998,    0, 9273,    2,    3,   72,  393,   33, 2123,    1,  146,\n",
       "            19,    6],\n",
       "         [1511,   18, 1441,    1,  846,  234,    1, 1380,    5, 1281,    7,\n",
       "          1643, 1082, 3870,   17,  380, 1352,    4,  207,    0,    1, 2610,\n",
       "             4,    1,  261,   13,    5,  335,    1,    2,   16,  764, 1502,\n",
       "            10,   42],\n",
       "         [1122,  644,   46,   20,    2, 1060,   82, 1092,  473,    6, 1916,\n",
       "             7,    2,    2,    8, 7588,   80,    6,    2, 2130,    7, 1933,\n",
       "             0, 5717,   82, 9028,  559,  549,    2,   22, 8662,    8,  537,\n",
       "             2,    0],\n",
       "         [  14,   93,   25, 1019,    5,  254,  170,   10,  207,    0,   54,\n",
       "          1453, 1260,   22, 1661,   15,    1,  468,   42,   45,   55, 1846,\n",
       "             1,   37,    9,  207,    4,  513,   12,    3,   48,    0,   14,\n",
       "            59,   79],\n",
       "         [   3,  394,   69,  123,    0,  271,  112,  610,    5, 3403,  206,\n",
       "             7, 3369,    4,   45,  309, 1660,    6, 3333,  354,    0,  367,\n",
       "             1,  332,  119,  745,  174,   90,  138, 2407,    1, 1243,    7,\n",
       "           820,  190],\n",
       "         [  53, 1525,  159,  727,   10,   23, 1384,    9,  217, 1290,   42,\n",
       "            34,    2, 1689,   16,  770,    9,  196,  907,    8,    1,  310,\n",
       "             4, 1644,    0,   38,  368,    1, 2435,  158,    4,  502,    8,\n",
       "           159,  770],\n",
       "         [ 120,   63,  309,   26,   32, 2683, 2703,  702,   18,  295,   63,\n",
       "           262, 2703,    0,    8,   57,  108,   87,  316,   10, 3064,   84,\n",
       "          7445,   13,   32, 8643,    4, 1023,  309,   41,  498, 6318, 7700,\n",
       "             5,    1],\n",
       "         [   2, 2185, 6738,    0,    1, 2185, 3531,   74, 3164,    1,    2,\n",
       "             3,   16,    6,    2, 8466,  170,    2,    2, 3574,    2,   23,\n",
       "          2661, 2552,   22, 8273,  325,    0,    1, 2185,  592,    2,    7,\n",
       "             1,  334],\n",
       "         [1048,  902,    1,  297,    4,  108,   56,   79, 4517,   11,   27,\n",
       "           992,    8, 5011,    2, 3483,   51,    2,  241,    0, 5011,  635,\n",
       "           294,   34, 2406,    5,   43,   12,    3,   48,   20,   12,    3,\n",
       "            48,   69],\n",
       "         [2638, 4075,  130, 1204,   38,   15,    0,   11,    1,    3, 1162,\n",
       "          1394,    1, 9139, 9034,   13, 5331,    6,  391,  377,    7,    1,\n",
       "           771,    2,    8, 3073, 1162,  696,   41,   79, 1048,  244,    1,\n",
       "           270,   11],\n",
       "         [4562,    4,  619,  184,   18,   47,   36,    2,  172,    0,  120,\n",
       "             1, 2202,   24,    7,  721,  131,  270,   42,  702,   25,  488,\n",
       "            17,    1, 2202,   36,  284,    5,    1,  500, 1696,    0,   39,\n",
       "           156, 1556],\n",
       "         [ 957, 8326, 2395,    0,   54, 3503,   24,    2,   84,    8,   27,\n",
       "          6743,    2,    0,  665,    1,   35, 3167, 1255,   17,    2,   65,\n",
       "           341,    1,    2, 1274, 2091,    1, 1474, 2205,  159, 2180,    4,\n",
       "          3256,    4],\n",
       "         [  16,    1, 2164, 1051,  532,    4,   71,    4,    1, 1404,  302,\n",
       "             8,  151,  154,    4,   27,  196,  222,    0,   28,   50,   58,\n",
       "            71,    4,    1,  196,  222,    0,   23, 2605,   59,   50,    2,\n",
       "          2281,   96],\n",
       "         [   5,  881,    5, 3120,   45, 3023,   28,   30,  556,    2, 3281,\n",
       "           112,   20,   12,    3,    5,   12,    3,    0,   29,    1,  187,\n",
       "          1446,  725, 1119,  270,    7,    1, 2317,   30,   58,    5,  629,\n",
       "           112,    8],\n",
       "         [   5, 2300,   43,    3,    3, 1141,  130,  855,    0, 3186, 1422,\n",
       "            80,   15,   14,  527,    6, 4568,  684,   22,    1,  125,    8,\n",
       "           111,  473,   11,    6,  273,  356,    4,  363,   21,  246,   70,\n",
       "             0,    1],\n",
       "         [ 164, 1880,  229,   10,    1, 2294,   30, 4708,    7,  310,   19,\n",
       "            31, 4904,   76,  654,    8,   61,  189,   89,   25,  263,    5,\n",
       "          5367,  692, 1370,    0,    2,   65,  660,    1,  129,  146, 1399,\n",
       "             4,  412],\n",
       "         [2438,   43,  690,    1,  225,   33,   59,  932, 2438,   10, 4068,\n",
       "             2,  196,   85,  754,   36,  129, 2754,   23,    2,   44,    0,\n",
       "          1211,  733,   80,    9,  564,   89, 6686,    1,  310,    4,   31,\n",
       "          1049,  305],\n",
       "         [  74,  285,   41,  372, 1662,  132,  301,  483,  568,  827,   15,\n",
       "            14,   42,  268,    1, 1334,    5, 2265,  581, 1585, 2656,    7,\n",
       "             1, 4479,    8, 1739,    0,   22,   10, 2274,    7, 1722,  164,\n",
       "            15,    1],\n",
       "         [   3,   38,   46, 4429,  382, 3238,    0,    1,  357, 2888,   27,\n",
       "           337,    2,    5, 3534, 7848,    8,  450,    5, 5323,  594,   10,\n",
       "           215,  357,    7,    1, 2588, 2569,    5, 1485,   27,  194, 2092,\n",
       "             0,   23],\n",
       "         [ 745,   20,    1,   12,    3,   21,    7,    1,  334,  109,    0,\n",
       "             8,   28, 1309,   10,   45, 1313, 1469,   13,  169,    7,    1,\n",
       "           130, 1552,    4, 2295,    0,    1,   37,  368,   27,  500,  253,\n",
       "           716,    5]]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, EMB_SIZE])\n",
    "\"\"\"\n",
    "tf.nn.embedding_lookup函数的用法主要是选取一个张量里面索引对应的元素。\n",
    "tf.nn.embedding_lookup（tensor, id）:tensor就是输入张量，id就是张量对应的索引，\n",
    "\"\"\"\n",
    "input_embedding = tf.nn.embedding_lookup(embedding, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"ptb.train\"          # 训练数据路径。\n",
    "EVAL_DATA = \"ptb.valid\"           # 验证数据路径。\n",
    "TEST_DATA = \"ptb.test\"            # 测试数据路径。\n",
    "HIDDEN_SIZE = 300                 # 隐藏层规模。\n",
    "NUM_LAYERS = 2                    # 深层循环神经网络中LSTM结构的层数。\n",
    "VOCAB_SIZE = 10000                # 词典规模。\n",
    "TRAIN_BATCH_SIZE = 20             # 训练数据batch的大小。\n",
    "TRAIN_NUM_STEP = 35               # 训练数据截断长度。\n",
    "\n",
    "EVAL_BATCH_SIZE = 1               # 测试数据batch的大小。\n",
    "EVAL_NUM_STEP = 1                 # 测试数据截断长度。\n",
    "NUM_EPOCH = 5                     # 使用训练数据的轮数。\n",
    "LSTM_KEEP_PROB = 0.9              # LSTM节点不被dropout的概率。\n",
    "EMBEDDING_KEEP_PROB = 0.9         # 词向量不被dropout的概率。\n",
    "MAX_GRAD_NORM = 5                 # 用于控制梯度膨胀的梯度大小上限。\n",
    "SHARE_EMB_AND_SOFTMAX = True      # 在Softmax层和词向量层之间共享参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        # 记录使用的batch大小和截断长度。\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        # 定义每一步的输入和预期输出。两者的维度都是[batch_size, num_steps]。\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 定义使用LSTM结构为循环体结构且使用dropout的深层循环神经网络。\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "            for _ in range(NUM_LAYERS)]     \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)            \n",
    "        \n",
    "        # 初始化最初的状态，即全零的向量。这个量只在每个epoch初始化第一个batch\n",
    "        # 时使用。\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # 定义单词的词向量矩阵。\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "        # 将输入单词转化为词向量。\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        \n",
    "        # 只在训练时使用dropout。\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)\n",
    "        \n",
    "        # 定义输出列表。在这里先将不同时刻LSTM结构的输出收集起来，再一起提供给\n",
    "        # softmax层。\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output) \n",
    "        # 把输出队列展开成[batch, hidden_size*num_steps]的形状，然后再\n",
    "        # reshape成[batch*numsteps, hidden_size]的形状。\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    "        \n",
    "        # Softmax层：将RNN在每个位置上的输出转化为各个单词的logits。\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            weight = tf.transpose(embedding)\n",
    "        else:\n",
    "            weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        # 定义交叉熵损失函数和平均损失。\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(self.targets, shape=[-1]),\n",
    "            logits=logits)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        #只在训练模型时定义反向传播操作\n",
    "        if not is_training: return\n",
    "        \n",
    "        #控制梯度大小，定义优化方法和训练步骤\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        \n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "                tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, batches, train_op, output_log, step):\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    \n",
    "    for x, y in batches:\n",
    "        cost, state, _ = session.run(\n",
    "                [model.cost, model.final_state, train_op],\n",
    "                {model.input_data: x, model.targets: y, model.initial_state: state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "        \n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After {} steps, perplexity is {}\".format(step, np.exp(total_costs / iters)))\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    return step, np.exp(total_costs / iters)\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        #将整个文档读进一个长字符串\n",
    "        id_string = ' '.join([line.strip() for line in fin.readlines()])\n",
    "    id_list = [int(w) for w in id_string.split()]\n",
    "    return id_list\n",
    "\n",
    "def make_batches(id_list, batch_size, num_step):\n",
    "    #计算总的batch数量 每个batch包含的单词数量是batch_size * num_step\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step)\n",
    "    \n",
    "    #将数据整理成一个维度为[batch_size, num_batches * num_step]的二维数组\n",
    "    data = np.array(id_list[: num_batches * batch_size * num_step])\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step])\n",
    "    #沿着第二个维度将数据切分成num_batches个batch,存入一个数组\n",
    "    data_batches = np.split(data, num_batches, axis=1)\n",
    "    \n",
    "    #重复上述操作，但是每个位置向右移动一位，这里得到的是RNN每一步输出所需要的下一个单词\n",
    "    label = np.array(id_list[1: num_batches * batch_size * num_step + 1])\n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)\n",
    "    \n",
    "    #返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵\n",
    "    return list(zip(data_batches, label_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, perplexity is 9986.092218903921\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-15d7d8b8d236>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-15d7d8b8d236>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"In iteration: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             step, train_pplx = run_epoch(sess, train_model, train_batches, \n\u001b[1;32m---> 21\u001b[1;33m                                          train_model.train_op, True, step)\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: {} Train Perplexity: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pplx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-f6592205e169>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(session, model, batches, train_op, output_log, step)\u001b[0m\n\u001b[0;32m      7\u001b[0m         cost, state, _ = session.run(\n\u001b[0;32m      8\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 {model.input_data: x, model.targets: y, model.initial_state: state})\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtotal_costs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0miters\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    \n",
    "    with tf.variable_scope(\"language_model\", reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "        \n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "    # 训练模型。\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        train_batches = make_batches(read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "        eval_batches = make_batches(read_data(EVAL_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "        test_batches = make_batches(read_data(TEST_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "        step = 0\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: {}\".format(i + 1))\n",
    "            step, train_pplx = run_epoch(sess, train_model, train_batches, \n",
    "                                         train_model.train_op, True, step)\n",
    "            print(\"Epoch: {} Train Perplexity: {}\".format(i + 1, train_pplx))\n",
    "            \n",
    "            _, eval_pplx = run_epoch(sess, eval_model, eval_batches, \n",
    "                                     tf.no_op(), False, 0)\n",
    "            print(\"Epoch: {} Eval Perplexity: {}\".format(i + 1, eval_pplx))\n",
    "            \n",
    "        _, test_pplx = run_epoch(sess, eval_model, test_batches,\n",
    "                                 tf.no_op(), False, 0)\n",
    "        print(\"Test Perplexity: {}\".format(test_pplx))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.reset_default_graph()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
